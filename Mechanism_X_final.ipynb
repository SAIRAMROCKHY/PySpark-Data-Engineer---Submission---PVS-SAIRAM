{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7012abc7-78df-4de8-a81c-7aad1321384c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col, current_timestamp, trim, lower, regexp_replace, expr\n",
    "import time\n",
    "import builtins  # for min function fix\n",
    "\n",
    "# Config\n",
    "CATALOG = \"testing\"\n",
    "SCHEMA = \"processed_data\"\n",
    "TRANSACTIONS_TABLE = \"transactions.financial_transactions_google_drive.transactions\"\n",
    "PROGRESS_TABLE = f\"{CATALOG}.{SCHEMA}.chunk_progress\"\n",
    "\n",
    "# ✅ Use volume path instead of raw S3 bucket\n",
    "VOLUME_PATH = \"/Volumes/testing/processed_data/staging_volume/staging_data/\"\n",
    "CHUNK_SIZE = 10000\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load full source table\n",
    "print(f\"Reading source table: {TRANSACTIONS_TABLE}\")\n",
    "df_full = spark.read.table(TRANSACTIONS_TABLE)\n",
    "max_row = df_full.count()\n",
    "print(f\"Total rows in source table: {max_row}\")\n",
    "\n",
    "if max_row == 0:\n",
    "    print(\"❌ Source table is empty. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Add row number column ordered by _line (adjust if needed)\n",
    "df_full = df_full.withColumn(\n",
    "    \"row_num\", \n",
    "    row_number().over(Window.partitionBy(lit(1)).orderBy(col(\"_line\")))\n",
    ")\n",
    "\n",
    "# Determine start row based on progress\n",
    "if spark.catalog.tableExists(PROGRESS_TABLE):\n",
    "    last_row_processed = spark.read.table(PROGRESS_TABLE).agg({\"last_row\": \"max\"}).collect()[0][0]\n",
    "    if last_row_processed >= max_row:\n",
    "        print(f\"Last processed row ({last_row_processed}) >= max rows ({max_row}), resetting start to 1\")\n",
    "        start = 1\n",
    "    else:\n",
    "        start = last_row_processed + 1\n",
    "    print(f\"Resuming from last processed row: {start}\")\n",
    "else:\n",
    "    start = 1\n",
    "    print(\"No progress table found, starting from row 1\")\n",
    "\n",
    "end = start + CHUNK_SIZE - 1\n",
    "\n",
    "while start <= max_row:\n",
    "    current_end = builtins.min(end, max_row)\n",
    "    print(f\"Processing chunk {start} to {current_end}\")\n",
    "    chunk_df = df_full.filter((col(\"row_num\") >= start) & (col(\"row_num\") <= current_end)).drop(\"row_num\")\n",
    "\n",
    "    # ✅ Clean & transform data\n",
    "    chunk_df = chunk_df \\\n",
    "        .withColumn(\"merchant\", trim(lower(regexp_replace(col(\"merchant\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "        .withColumn(\"customer\", trim(lower(regexp_replace(col(\"customer\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "        .withColumn(\"category\", trim(lower(regexp_replace(col(\"category\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "        .withColumn(\"gender\", trim(lower(regexp_replace(col(\"gender\"), \"[^a-zA-Z]\", \"\")))) \\\n",
    "        .withColumn(\"age\", expr(\"try_cast(regexp_replace(age, '[^0-9]', '') AS int)\")) \\\n",
    "        .withColumn(\"amount\", expr(\"try_cast(regexp_replace(amount, '[^0-9.]', '') AS double)\")) \\\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "        .dropna(subset=[\"merchant\", \"customer\", \"amount\"])\n",
    "\n",
    "    # Add event timestamp\n",
    "    chunk_df = chunk_df.withColumn(\"event_time\", current_timestamp())\n",
    "\n",
    "    try:\n",
    "        # ✅ Write using Unity Catalog volume path\n",
    "        chunk_df.write.format(\"delta\").mode(\"append\").save(VOLUME_PATH)\n",
    "        print(f\"✅ Wrote chunk {start}-{current_end} at {time.strftime('%Y-%m-%d %H:%M:%S')} to volume path\")\n",
    "\n",
    "        # Save progress to tracking table\n",
    "        progress_df = spark.createDataFrame([(current_end,)], [\"last_row\"])\n",
    "        progress_df.write.mode(\"overwrite\").saveAsTable(PROGRESS_TABLE)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error writing chunk {start}-{current_end} to volume path: {e}\")\n",
    "        break\n",
    "\n",
    "    time.sleep(10)\n",
    "    start += CHUNK_SIZE\n",
    "    end += CHUNK_SIZE\n",
    "\n",
    "print(\"\\nDone writing chunks. Verifying written data sample:\")\n",
    "try:\n",
    "    spark.read.format(\"delta\").load(VOLUME_PATH).limit(10).show()\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error reading chunk output path: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db721a9-8a4e-4960-a448-13b391de6e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/Volumes/testing/processed_data/staging_volume/staging_data/\")\n",
    "df.select(\"amount\", \"merchant\", \"customer\", \"category\").summary().show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mechanism_X_final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
