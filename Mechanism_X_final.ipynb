{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b5f7a95-fb9e-42a7-9a10-4ab411ebfff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mechanism X structured streaming code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7012abc7-78df-4de8-a81c-7aad1321384c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, col, current_timestamp, trim, lower, regexp_replace, expr, lit\n",
    "from pyspark.sql.window import Window\n",
    "import builtins\n",
    "\n",
    "# Config\n",
    "CATALOG = \"raw\"\n",
    "SCHEMA = \"staging\"\n",
    "TRANSACTIONS_TABLE = \"googledrive.raw.transactions\"\n",
    "VOLUME_PATH = \"/Volumes/raw/staging/staging_volume/staging_data/\"\n",
    "CHUNK_SIZE = 10000\n",
    "CHECKPOINT_PATH = \"/Volumes/raw/staging/checkpoints/mechanism_x_streaming/\"\n",
    "\n",
    "# Read source table as streaming (simulate new rows appended)\n",
    "# If the source table is a Delta table with new data appending, this works well\n",
    "df_stream = spark.readStream.format(\"delta\").table(TRANSACTIONS_TABLE)\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    if batch_df.count() == 0:\n",
    "        print(f\"Batch {batch_id}: No new data to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Batch {batch_id}: Processing {batch_df.count()} rows\")\n",
    "\n",
    "    # Add row number for chunking within batch\n",
    "    window_spec = Window.orderBy(\"_line\")\n",
    "    batch_df = batch_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "    total_rows = batch_df.count()\n",
    "    start = 1\n",
    "    while start <= total_rows:\n",
    "        end = builtins.min(start + CHUNK_SIZE - 1, total_rows)\n",
    "\n",
    "        chunk_df = batch_df.filter((col(\"row_num\") >= start) & (col(\"row_num\") <= end)).drop(\"row_num\")\n",
    "\n",
    "        # Clean & transform data\n",
    "        chunk_df = chunk_df \\\n",
    "            .withColumn(\"merchant\", trim(lower(regexp_replace(col(\"merchant\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "            .withColumn(\"customer\", trim(lower(regexp_replace(col(\"customer\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "            .withColumn(\"category\", trim(lower(regexp_replace(col(\"category\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "            .withColumn(\"gender\", trim(lower(regexp_replace(col(\"gender\"), \"[^a-zA-Z]\", \"\")))) \\\n",
    "            .withColumn(\"age\", expr(\"try_cast(regexp_replace(age, '[^0-9]', '') AS int)\")) \\\n",
    "            .withColumn(\"amount\", expr(\"try_cast(regexp_replace(amount, '[^0-9.]', '') AS double)\")) \\\n",
    "            .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "            .dropna(subset=[\"merchant\", \"customer\", \"amount\"])\n",
    "\n",
    "        chunk_df = chunk_df.withColumn(\"event_time\", current_timestamp())\n",
    "\n",
    "        try:\n",
    "            chunk_df.write.format(\"delta\").mode(\"append\").save(VOLUME_PATH)\n",
    "            print(f\"Batch {batch_id}: Wrote chunk rows {start} to {end}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Batch {batch_id}: Error writing chunk rows {start} to {end}: {e}\")\n",
    "            break\n",
    "\n",
    "        start += CHUNK_SIZE\n",
    "\n",
    "# Start streaming query with foreachBatch to chunk & write batches\n",
    "query = df_stream.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH) \\\n",
    "    .trigger(processingTime=\"1 second\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3cb6f83-1d52-425c-9327-fcd40fa90e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b508665b-ffb7-47d2-ae8f-c9e81b2dcffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a flag table for Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c27f36-1bc1-4bf7-953f-c132e6387e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# CREATE TABLE IF NOT EXISTS raw.staging.mechanism_flag (\n",
    "#   status STRING\n",
    "# );\n",
    "\n",
    "# INSERT INTO raw.staging.mechanism_flag VALUES ('not_ready');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aae37618-6bc2-40cc-aec9-8176924900d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### Mechanism X code for Workflows and Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62e9bebc-8923-48de-ae83-78d1e79678ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import builtins\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€\n",
    "CATALOG = \"raw\"\n",
    "TRANSACTIONS_TABLE = \"googledrive.raw.transactions\"\n",
    "CHUNK_OUTPUT_PATH = \"/Volumes/raw/staging/staging_volume/staging_data/\"\n",
    "CHUNK_SIZE = 10000\n",
    "CHECKPOINT_PATH = \"/Volumes/raw/staging/checkpoints/mechanism_x_streaming/\"\n",
    "FLAG_TABLE = \"raw.staging.mechanism_flag\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ Read Stream â”€â”€â”€â”€â”€â”€\n",
    "df_stream = spark.readStream.format(\"delta\").table(TRANSACTIONS_TABLE)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ Batch Processing â”€â”€â”€â”€â”€â”€\n",
    "def process_batch(batch_df, batch_id):\n",
    "    if batch_df.count() == 0:\n",
    "        print(f\"Batch {batch_id}: No new data.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Batch {batch_id}: Processing {batch_df.count()} rows\")\n",
    "\n",
    "    window_spec = Window.orderBy(\"_line\")\n",
    "    batch_df = batch_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "    total_rows = batch_df.count()\n",
    "    start = 1\n",
    "    wrote_flag = False\n",
    "\n",
    "    while start <= total_rows:\n",
    "        end = builtins.min(start + CHUNK_SIZE - 1, total_rows)\n",
    "\n",
    "        chunk_df = batch_df.filter((col(\"row_num\") >= start) & (col(\"row_num\") <= end)).drop(\"row_num\")\n",
    "\n",
    "        # Clean columns\n",
    "        chunk_df = chunk_df \\\n",
    "            .withColumn(\"merchant\", trim(lower(regexp_replace(col(\"merchant\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "            .withColumn(\"customer\", trim(lower(regexp_replace(col(\"customer\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "            .withColumn(\"category\", trim(lower(regexp_replace(col(\"category\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "            .withColumn(\"gender\", trim(lower(regexp_replace(col(\"gender\"), \"[^a-zA-Z]\", \"\")))) \\\n",
    "            .withColumn(\"age\", expr(\"try_cast(regexp_replace(age, '[^0-9]', '') AS int)\")) \\\n",
    "            .withColumn(\"amount\", expr(\"try_cast(regexp_replace(amount, '[^0-9.]', '') AS double)\")) \\\n",
    "            .withColumn(\"event_time\", current_timestamp()) \\\n",
    "            .dropna(subset=[\"merchant\", \"customer\", \"amount\"])\n",
    "\n",
    "        # Write chunk\n",
    "        chunk_df.write.format(\"delta\").mode(\"append\").save(CHUNK_OUTPUT_PATH)\n",
    "        print(f\"âœ… Batch {batch_id}: Wrote rows {start} to {end}\")\n",
    "\n",
    "        if not wrote_flag:\n",
    "            # Check if the flag table exists before updating\n",
    "            if spark._jsparkSession.catalog().tableExists(FLAG_TABLE):\n",
    "                spark.sql(f\"UPDATE {FLAG_TABLE} SET status = 'ready'\")\n",
    "                print(\"ðŸš© Flag updated in Delta table: status = 'ready'\")\n",
    "            else:\n",
    "                print(f\"ðŸš© Flag table {FLAG_TABLE} does not exist.\")\n",
    "            wrote_flag = True\n",
    "\n",
    "        start += CHUNK_SIZE\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ Start Streaming â”€â”€â”€â”€â”€â”€\n",
    "query = df_stream.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH) \\\n",
    "    .trigger(processingTime=\"1 second\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39bf1b0d-1833-4262-9a99-31800d7a18ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verifying the writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db721a9-8a4e-4960-a448-13b391de6e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/Volumes/raw/staging/staging_volume/staging_data/\")\n",
    "df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mechanism_X_final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
