{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b5f7a95-fb9e-42a7-9a10-4ab411ebfff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mechanism X structured streaming code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7012abc7-78df-4de8-a81c-7aad1321384c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, col, current_timestamp, trim, lower, regexp_replace, expr, lit\n",
    "from pyspark.sql.window import Window\n",
    "import builtins\n",
    "\n",
    "# Config\n",
    "CATALOG = \"raw\"\n",
    "SCHEMA = \"staging\"\n",
    "TRANSACTIONS_TABLE = \"googledrive.raw.transactions\"\n",
    "VOLUME_PATH = \"/Volumes/raw/staging/staging_volume/staging_data/\"\n",
    "CHUNK_SIZE = 10000\n",
    "CHECKPOINT_PATH = \"/Volumes/raw/staging/checkpoints/mechanism_x_streaming/\"\n",
    "\n",
    "# Read source table as streaming (simulate new rows appended)\n",
    "# If the source table is a Delta table with new data appending, this works well\n",
    "df_stream = spark.readStream.format(\"delta\").table(TRANSACTIONS_TABLE)\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    if batch_df.count() == 0:\n",
    "        print(f\"Batch {batch_id}: No new data to process.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Batch {batch_id}: Processing {batch_df.count()} rows\")\n",
    "\n",
    "    # Add row number for chunking within batch\n",
    "    window_spec = Window.orderBy(\"_line\")\n",
    "    batch_df = batch_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "    total_rows = batch_df.count()\n",
    "    start = 1\n",
    "    while start <= total_rows:\n",
    "        end = builtins.min(start + CHUNK_SIZE - 1, total_rows)\n",
    "\n",
    "        chunk_df = batch_df.filter((col(\"row_num\") >= start) & (col(\"row_num\") <= end)).drop(\"row_num\")\n",
    "\n",
    "        # Clean & transform data\n",
    "        chunk_df = chunk_df \\\n",
    "            .withColumn(\"merchant\", trim(lower(regexp_replace(col(\"merchant\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "            .withColumn(\"customer\", trim(lower(regexp_replace(col(\"customer\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "            .withColumn(\"category\", trim(lower(regexp_replace(col(\"category\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "            .withColumn(\"gender\", trim(lower(regexp_replace(col(\"gender\"), \"[^a-zA-Z]\", \"\")))) \\\n",
    "            .withColumn(\"age\", expr(\"try_cast(regexp_replace(age, '[^0-9]', '') AS int)\")) \\\n",
    "            .withColumn(\"amount\", expr(\"try_cast(regexp_replace(amount, '[^0-9.]', '') AS double)\")) \\\n",
    "            .withColumn(\"ingestion_timestamp\", current_timestamp()) \\\n",
    "            .dropna(subset=[\"merchant\", \"customer\", \"amount\"])\n",
    "\n",
    "        chunk_df = chunk_df.withColumn(\"event_time\", current_timestamp())\n",
    "\n",
    "        try:\n",
    "            chunk_df.write.format(\"delta\").mode(\"append\").save(VOLUME_PATH)\n",
    "            print(f\"Batch {batch_id}: Wrote chunk rows {start} to {end}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Batch {batch_id}: Error writing chunk rows {start} to {end}: {e}\")\n",
    "            break\n",
    "\n",
    "        start += CHUNK_SIZE\n",
    "\n",
    "# Start streaming query with foreachBatch to chunk & write batches\n",
    "query = df_stream.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH) \\\n",
    "    .trigger(processingTime=\"1 second\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39bf1b0d-1833-4262-9a99-31800d7a18ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verifying the writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3db721a9-8a4e-4960-a448-13b391de6e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/Volumes/raw/staging/staging_volume/staging_data/\")\n",
    "df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mechanism_X_final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
