{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66d16742-c462-485d-b4cd-7c359a3044a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mechanism Y structured streaming code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be3b9e50-8d81-4bd3-a1fb-7346a06fec95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "INPUT_PATH = \"/Volumes/raw/staging/staging_volume/staging_data/\"\n",
    "OUTPUT_PATH = \"/Volumes/processed/detection_data/processed_volume/output_detections_data/\"\n",
    "IMPORTANCE_TABLE = \"googledrive.raw.customer_importance\"\n",
    "CHECKPOINT_PATH = \"/Volumes/processed/detection_data/checkpoints/mechanism_y_streaming/\"\n",
    "\n",
    "# ----------------------------\n",
    "# Load importance reference\n",
    "# ----------------------------\n",
    "imp_df = spark.read.table(IMPORTANCE_TABLE) \\\n",
    "    .select(\"source\", \"target\", \"type_trans\", \"weight\") \\\n",
    "    .withColumn(\"source\", trim(lower(regexp_replace(col(\"source\"), \"'\", \"\")))) \\\n",
    "    .withColumn(\"target\", trim(lower(regexp_replace(col(\"target\"), \"'\", \"\")))) \\\n",
    "    .withColumn(\"type_trans\", trim(lower(regexp_replace(col(\"type_trans\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "    .withColumn(\"weight\", col(\"weight\").cast(\"double\")) \\\n",
    "    .dropna().cache()\n",
    "\n",
    "# ----------------------------\n",
    "# Utility\n",
    "# ----------------------------\n",
    "def ist_now():\n",
    "    return datetime.now(pytz.timezone(\"Asia/Kolkata\")).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# ----------------------------\n",
    "# Pattern Detection Logic\n",
    "# ----------------------------\n",
    "def detect_patterns(df):\n",
    "    ystart = ist_now()\n",
    "\n",
    "    # Pattern 1: UPGRADE\n",
    "    joined = df.join(imp_df,\n",
    "        (df[\"merchant\"] == imp_df[\"target\"]) &\n",
    "        (df[\"customer\"] == imp_df[\"source\"]) &\n",
    "        (df[\"category\"] == imp_df[\"type_trans\"]),\n",
    "        \"inner\"\n",
    "    )\n",
    "\n",
    "    agg = joined.groupBy(\"merchant\", \"customer\") \\\n",
    "        .agg(count(\"*\").alias(\"txn_count\"),\n",
    "             avg(\"weight\").alias(\"avg_weight\"))\n",
    "\n",
    "    total_txns = joined.groupBy(\"merchant\") \\\n",
    "        .agg(count(\"*\").alias(\"total_txns\"))\n",
    "\n",
    "    p1_df = agg.join(total_txns, \"merchant\") \\\n",
    "        .filter((col(\"total_txns\") >= 50000) &\n",
    "                (col(\"txn_count\") >= 169) &\n",
    "                (col(\"avg_weight\") <= 23)) \\\n",
    "        .select(\n",
    "            lit(ystart).alias(\"YStartTime\"),\n",
    "            current_timestamp().alias(\"detectionTime\"),\n",
    "            lit(\"PatId1\").alias(\"patternId\"),\n",
    "            lit(\"UPGRADE\").alias(\"ActionType\"),\n",
    "            col(\"customer\").alias(\"customerName\"),\n",
    "            col(\"merchant\").alias(\"merchantId\")\n",
    "        )\n",
    "\n",
    "    print(f\"ðŸ” Pattern 1 - UPGRADE: {p1_df.count()} rows detected.\")\n",
    "\n",
    "    # Pattern 2: CHILD\n",
    "    p2_df = df.groupBy(\"merchant\", \"customer\") \\\n",
    "        .agg(avg(\"amount\").alias(\"avg_amount\"),\n",
    "             count(\"*\").alias(\"txn_count\")) \\\n",
    "        .filter((col(\"avg_amount\") < 23) &\n",
    "                (col(\"txn_count\") >= 8)) \\\n",
    "        .select(\n",
    "            lit(ystart).alias(\"YStartTime\"),\n",
    "            current_timestamp().alias(\"detectionTime\"),\n",
    "            lit(\"PatId2\").alias(\"patternId\"),\n",
    "            lit(\"CHILD\").alias(\"ActionType\"),\n",
    "            col(\"customer\").alias(\"customerName\"),\n",
    "            col(\"merchant\").alias(\"merchantId\")\n",
    "        )\n",
    "\n",
    "    print(f\"ðŸ” Pattern 2 - CHILD: {p2_df.count()} rows detected.\")\n",
    "\n",
    "    # Pattern 3: DEI-NEEDED\n",
    "    gender_df = df.withColumn(\"gender_cleaned\", regexp_replace(col(\"gender\"), \"'\", \"\")) \\\n",
    "                  .withColumn(\"gender_cleaned\", trim(lower(col(\"gender_cleaned\")))) \\\n",
    "                  .withColumn(\"gender_norm\",\n",
    "                      when(col(\"gender_cleaned\").like(\"m%\"), \"Male\")\n",
    "                      .when(col(\"gender_cleaned\").like(\"f%\"), \"Female\")\n",
    "                      .otherwise(None)\n",
    "                  )\n",
    "\n",
    "    p3_raw = gender_df.dropna(subset=[\"gender_norm\", \"merchant\", \"customer\"]) \\\n",
    "        .select(\"merchant\", \"customer\", \"gender_norm\") \\\n",
    "        .distinct() \\\n",
    "        .groupBy(\"merchant\") \\\n",
    "        .pivot(\"gender_norm\", [\"Male\", \"Female\"]) \\\n",
    "        .count().na.fill(0)\n",
    "\n",
    "    p3_df = p3_raw.filter((col(\"Female\") >= 100) & (col(\"Female\") < col(\"Male\"))) \\\n",
    "        .select(\n",
    "            lit(ystart).alias(\"YStartTime\"),\n",
    "            current_timestamp().alias(\"detectionTime\"),\n",
    "            lit(\"PatId3\").alias(\"patternId\"),\n",
    "            lit(\"DEI-NEEDED\").alias(\"ActionType\"),\n",
    "            lit(\"\").alias(\"customerName\"),\n",
    "            col(\"merchant\").alias(\"merchantId\")\n",
    "        )\n",
    "\n",
    "    print(f\"ðŸ” Pattern 3 - DEI-NEEDED: {p3_df.count()} rows detected.\")\n",
    "\n",
    "    # Union all\n",
    "    def empty_detection_df():\n",
    "        return spark.createDataFrame([], schema=\"\"\"\n",
    "            YStartTime STRING,\n",
    "            detectionTime TIMESTAMP,\n",
    "            patternId STRING,\n",
    "            ActionType STRING,\n",
    "            customerName STRING,\n",
    "            merchantId STRING\n",
    "        \"\"\")\n",
    "\n",
    "    final_df = (\n",
    "        (p1_df if not p1_df.rdd.isEmpty() else empty_detection_df())\n",
    "        .unionByName(p2_df if not p2_df.rdd.isEmpty() else empty_detection_df())\n",
    "        .unionByName(p3_df if not p3_df.rdd.isEmpty() else empty_detection_df())\n",
    "    )\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# ----------------------------\n",
    "# Streaming Ingestion\n",
    "# ----------------------------\n",
    "stream = spark.readStream.format(\"delta\").load(INPUT_PATH)\n",
    "\n",
    "# Clean the input\n",
    "cleaned = stream \\\n",
    "    .withColumn(\"merchant\", trim(lower(regexp_replace(col(\"merchant\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "    .withColumn(\"customer\", trim(lower(regexp_replace(col(\"customer\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "    .withColumn(\"category\", trim(lower(regexp_replace(col(\"category\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "    .withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "    .withColumn(\"gender\", col(\"gender\")) \\\n",
    "    .dropna(subset=[\"merchant\", \"customer\"])\n",
    "\n",
    "# ----------------------------\n",
    "# foreachBatch Logic\n",
    "# ----------------------------\n",
    "def process_batch(micro_df, batch_id):\n",
    "    if micro_df.isEmpty():\n",
    "        print(f\"ðŸŸ¡ Base batch {batch_id} contains no rows.\")\n",
    "        return\n",
    "\n",
    "    micro_df.persist()\n",
    "    detections = detect_patterns(micro_df)\n",
    "\n",
    "    if not detections.rdd.isEmpty():\n",
    "        detections = detections.withColumn(\"file_id\", floor(monotonically_increasing_id() / 50))\n",
    "        detections.write.format(\"delta\").mode(\"append\").partitionBy(\"file_id\").save(OUTPUT_PATH)\n",
    "        print(f\"âœ… Batch {batch_id}: {detections.count()} detections saved.\")\n",
    "    else:\n",
    "        print(f\"âšª Batch {batch_id}: No pattern matches.\")\n",
    "\n",
    "    micro_df.unpersist()\n",
    "\n",
    "# ----------------------------\n",
    "# Start Streaming Query\n",
    "# ----------------------------\n",
    "query = cleaned.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH) \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83556180-1390-4758-95bb-639dc29764b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Mechanism Y code for Workflows and Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98bc7b8c-6392-43dd-8901-4a1415fb31e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import time\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ Spark Session â”€â”€â”€â”€â”€â”€\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€\n",
    "INPUT_PATH = \"/Volumes/raw/staging/staging_volume/staging_data/\"\n",
    "OUTPUT_PATH = \"/Volumes/processed/detection_data/processed_volume/output_detections_data/\"\n",
    "IMPORTANCE_TABLE = \"googledrive.raw.customer_importance\"\n",
    "CHECKPOINT_PATH = \"/Volumes/processed/detection_data/checkpoints/mechanism_y_streaming/\"\n",
    "FLAG_TABLE = \"raw.staging.mechanism_flag\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ Wait for Flag â”€â”€â”€â”€â”€â”€\n",
    "def is_flag_ready():\n",
    "    try:\n",
    "        return spark.sql(f\"SELECT status FROM {FLAG_TABLE}\").collect()[0][\"status\"] == \"ready\"\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "print(\"â³ Waiting for Mechanism X to signal readiness via Delta flag table...\")\n",
    "while not is_flag_ready():\n",
    "    print(\"ðŸ” Still waiting for flag = 'ready'...\")\n",
    "    time.sleep(10)\n",
    "print(\"âœ… Flag detected! Starting Mechanism Y...\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ Load Importance Reference â”€â”€â”€â”€â”€â”€\n",
    "imp_df = spark.read.table(IMPORTANCE_TABLE) \\\n",
    "    .select(\"source\", \"target\", \"type_trans\", \"weight\") \\\n",
    "    .withColumn(\"source\", trim(lower(regexp_replace(col(\"source\"), \"'\", \"\")))) \\\n",
    "    .withColumn(\"target\", trim(lower(regexp_replace(col(\"target\"), \"'\", \"\")))) \\\n",
    "    .withColumn(\"type_trans\", trim(lower(regexp_replace(col(\"type_trans\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "    .withColumn(\"weight\", col(\"weight\").cast(\"double\")) \\\n",
    "    .dropna().cache()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ Time Helper â”€â”€â”€â”€â”€â”€\n",
    "def ist_now():\n",
    "    return datetime.now(pytz.timezone(\"Asia/Kolkata\")).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ Pattern Detection â”€â”€â”€â”€â”€â”€\n",
    "def detect_patterns(df):\n",
    "    ystart = ist_now()\n",
    "\n",
    "    # Pattern 1: UPGRADE\n",
    "    joined = df.join(imp_df,\n",
    "        (df[\"merchant\"] == imp_df[\"target\"]) &\n",
    "        (df[\"customer\"] == imp_df[\"source\"]) &\n",
    "        (df[\"category\"] == imp_df[\"type_trans\"]),\n",
    "        \"inner\"\n",
    "    )\n",
    "\n",
    "    agg = joined.groupBy(\"merchant\", \"customer\") \\\n",
    "        .agg(count(\"*\").alias(\"txn_count\"),\n",
    "             avg(\"weight\").alias(\"avg_weight\"))\n",
    "\n",
    "    total_txns = joined.groupBy(\"merchant\") \\\n",
    "        .agg(count(\"*\").alias(\"total_txns\"))\n",
    "\n",
    "    p1_df = agg.join(total_txns, \"merchant\") \\\n",
    "        .filter((col(\"total_txns\") >= 50000) &\n",
    "                (col(\"txn_count\") >= 169) &\n",
    "                (col(\"avg_weight\") <= 23)) \\\n",
    "        .select(\n",
    "            lit(ystart).alias(\"YStartTime\"),\n",
    "            current_timestamp().alias(\"detectionTime\"),\n",
    "            lit(\"PatId1\").alias(\"patternId\"),\n",
    "            lit(\"UPGRADE\").alias(\"ActionType\"),\n",
    "            col(\"customer\").alias(\"customerName\"),\n",
    "            col(\"merchant\").alias(\"merchantId\")\n",
    "        )\n",
    "\n",
    "    print(f\"ðŸ” Pattern 1 - UPGRADE: {p1_df.count()} rows detected.\")\n",
    "\n",
    "    # Pattern 2: CHILD\n",
    "    p2_df = df.groupBy(\"merchant\", \"customer\") \\\n",
    "        .agg(avg(\"amount\").alias(\"avg_amount\"),\n",
    "             count(\"*\").alias(\"txn_count\")) \\\n",
    "        .filter((col(\"avg_amount\") < 23) &\n",
    "                (col(\"txn_count\") >= 8)) \\\n",
    "        .select(\n",
    "            lit(ystart).alias(\"YStartTime\"),\n",
    "            current_timestamp().alias(\"detectionTime\"),\n",
    "            lit(\"PatId2\").alias(\"patternId\"),\n",
    "            lit(\"CHILD\").alias(\"ActionType\"),\n",
    "            col(\"customer\").alias(\"customerName\"),\n",
    "            col(\"merchant\").alias(\"merchantId\")\n",
    "        )\n",
    "\n",
    "    print(f\"ðŸ” Pattern 2 - CHILD: {p2_df.count()} rows detected.\")\n",
    "\n",
    "    # Pattern 3: DEI-NEEDED\n",
    "    gender_df = df.withColumn(\"gender_cleaned\", regexp_replace(col(\"gender\"), \"'\", \"\")) \\\n",
    "                  .withColumn(\"gender_cleaned\", trim(lower(col(\"gender_cleaned\")))) \\\n",
    "                  .withColumn(\"gender_norm\",\n",
    "                      when(col(\"gender_cleaned\").like(\"m%\"), \"Male\")\n",
    "                      .when(col(\"gender_cleaned\").like(\"f%\"), \"Female\")\n",
    "                      .otherwise(None)\n",
    "                  )\n",
    "\n",
    "    p3_raw = gender_df.dropna(subset=[\"gender_norm\", \"merchant\", \"customer\"]) \\\n",
    "        .select(\"merchant\", \"customer\", \"gender_norm\") \\\n",
    "        .distinct() \\\n",
    "        .groupBy(\"merchant\") \\\n",
    "        .pivot(\"gender_norm\", [\"Male\", \"Female\"]) \\\n",
    "        .count().na.fill(0)\n",
    "\n",
    "    p3_df = p3_raw.filter((col(\"Female\") >= 100) & (col(\"Female\") < col(\"Male\"))) \\\n",
    "        .select(\n",
    "            lit(ystart).alias(\"YStartTime\"),\n",
    "            current_timestamp().alias(\"detectionTime\"),\n",
    "            lit(\"PatId3\").alias(\"patternId\"),\n",
    "            lit(\"DEI-NEEDED\").alias(\"ActionType\"),\n",
    "            lit(\"\").alias(\"customerName\"),\n",
    "            col(\"merchant\").alias(\"merchantId\")\n",
    "        )\n",
    "\n",
    "    print(f\"ðŸ” Pattern 3 - DEI-NEEDED: {p3_df.count()} rows detected.\")\n",
    "\n",
    "    # Union all\n",
    "    def empty_detection_df():\n",
    "        return spark.createDataFrame([], schema=\"\"\"\n",
    "            YStartTime STRING,\n",
    "            detectionTime TIMESTAMP,\n",
    "            patternId STRING,\n",
    "            ActionType STRING,\n",
    "            customerName STRING,\n",
    "            merchantId STRING\n",
    "        \"\"\")\n",
    "\n",
    "    final_df = (\n",
    "        (p1_df if not p1_df.rdd.isEmpty() else empty_detection_df())\n",
    "        .unionByName(p2_df if not p2_df.rdd.isEmpty() else empty_detection_df())\n",
    "        .unionByName(p3_df if not p3_df.rdd.isEmpty() else empty_detection_df())\n",
    "    )\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ Streaming Input â”€â”€â”€â”€â”€â”€\n",
    "stream = spark.readStream.format(\"delta\").load(INPUT_PATH)\n",
    "\n",
    "cleaned = stream \\\n",
    "    .withColumn(\"merchant\", trim(lower(regexp_replace(col(\"merchant\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "    .withColumn(\"customer\", trim(lower(regexp_replace(col(\"customer\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "    .withColumn(\"category\", trim(lower(regexp_replace(col(\"category\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "    .withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "    .withColumn(\"gender\", col(\"gender\")) \\\n",
    "    .dropna(subset=[\"merchant\", \"customer\"])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ foreachBatch â”€â”€â”€â”€â”€â”€\n",
    "def process_batch(micro_df, batch_id):\n",
    "    if micro_df.isEmpty():\n",
    "        print(f\"ðŸŸ¡ Batch {batch_id}: No new rows.\")\n",
    "        return\n",
    "\n",
    "    micro_df.persist()\n",
    "    detections = detect_patterns(micro_df)\n",
    "\n",
    "    if not detections.rdd.isEmpty():\n",
    "        detections = detections.withColumn(\"file_id\", floor(monotonically_increasing_id() / 50))\n",
    "        detections.write.format(\"delta\").mode(\"append\").partitionBy(\"file_id\").save(OUTPUT_PATH)\n",
    "        print(f\"âœ… Batch {batch_id}: {detections.count()} detections written.\")\n",
    "    else:\n",
    "        print(f\"âšª Batch {batch_id}: No detections.\")\n",
    "\n",
    "    micro_df.unpersist()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ Start Streaming Query â”€â”€â”€â”€â”€â”€\n",
    "query = cleaned.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH) \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccd5e18d-549a-42d4-b729-97068da1c526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Verifying the writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41024aff-baa5-4e6e-8902-6345edd7fbb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/Volumes/processed/detection_data/processed_volume/output_detections_data/\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c282f7e-5c80-4a0f-b0e5-44824b815114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run this code before running the Mechanisms to get thresholds in hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234045ab-5f73-450e-9ec6-5666051f6c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, expr\n",
    "\n",
    "\n",
    "# Load cleaned transaction data\n",
    "transactions = spark.read.table(\"googledrive.raw.transactions\") \n",
    "\n",
    "# Load cleaned importance reference data\n",
    "imp_df = spark.read.table(\"googledrive.raw.customer_importance\") \n",
    "\n",
    "# Join transaction and importance reference\n",
    "joined = transactions.join(\n",
    "    imp_df,\n",
    "    (transactions[\"merchant\"] == imp_df[\"target\"]) &\n",
    "    (transactions[\"customer\"] == imp_df[\"source\"]) &\n",
    "    (transactions[\"category\"] == imp_df[\"type_trans\"]),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Aggregate txn_count and avg_weight per (merchant, customer)\n",
    "agg = joined.groupBy(\"merchant\", \"customer\").agg(\n",
    "    count(\"*\").alias(\"txn_count\"),\n",
    "    avg(\"weight\").alias(\"avg_weight\")\n",
    ")\n",
    "\n",
    "# Total transactions per merchant\n",
    "total_txns = joined.groupBy(\"merchant\").agg(count(\"*\").alias(\"total_txns\"))\n",
    "\n",
    "# Filter merchants with at least 50,000 total transactions\n",
    "agg = agg.join(total_txns, \"merchant\").filter(col(\"total_txns\") >= 50000)\n",
    "\n",
    "# Precompute thresholds per merchant\n",
    "thresholds_df = agg.groupBy(\"merchant\").agg(\n",
    "    expr(\"percentile_approx(txn_count, 0.9)\").alias(\"txn_count_90\"),\n",
    "    expr(\"percentile_approx(avg_weight, 0.1)\").alias(\"avg_weight_10\")\n",
    ")\n",
    "\n",
    "thresholds_df.display()\n",
    "# # Save the thresholds for real-time use in streaming pipeline\n",
    "# thresholds_df.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/processed/merchant_thresholds/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9329b106-969e-4078-be7f-c92a1ec6e628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pattern 1 - Debugging code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71e83794-aca0-424f-99b8-92fa5ee27a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, trim, lower, regexp_replace\n",
    "\n",
    "# Load streaming batch or batch static data for debugging\n",
    "df = spark.read.table(\"googledrive.raw.transactions\")\n",
    "\n",
    "# Load importance lookup once\n",
    "imp_df = spark.read.table(\"googledrive.raw.customer_importance\") \\\n",
    "    .select(\"source\", \"target\", \"type_trans\", \"weight\") \\\n",
    "    .withColumn(\"source\", trim(lower(regexp_replace(col(\"source\"), \"'\", \"\")))) \\\n",
    "    .withColumn(\"target\", trim(lower(regexp_replace(col(\"target\"), \"'\", \"\")))) \\\n",
    "    .withColumn(\"type_trans\", trim(lower(regexp_replace(col(\"type_trans\"), \"'\", \"\")))) \\\n",
    "    .withColumn(\"type_trans\", regexp_replace(col(\"type_trans\"), \"_\", \"\")) \\\n",
    "    .withColumn(\"weight\", col(\"weight\").cast(\"double\")) \\\n",
    "    .dropna() \\\n",
    "    .cache()\n",
    "\n",
    "# Clean input columns for join keys\n",
    "df_clean = df.withColumn(\"merchant\", trim(lower(regexp_replace(col(\"merchant\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "             .withColumn(\"customer\", trim(lower(regexp_replace(col(\"customer\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "             .withColumn(\"category\", trim(lower(regexp_replace(col(\"category\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "             .dropna(subset=[\"merchant\", \"customer\"])\n",
    "\n",
    "# Join transactions with importance data\n",
    "joined = df_clean.join(imp_df,\n",
    "    (df_clean[\"merchant\"] == imp_df[\"target\"]) &\n",
    "    (df_clean[\"customer\"] == imp_df[\"source\"]) &\n",
    "    (df_clean[\"category\"] == imp_df[\"type_trans\"]),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# Aggregate metrics per (merchant, customer)\n",
    "agg = joined.groupBy(\"merchant\", \"customer\") \\\n",
    "            .agg(count(\"*\").alias(\"txn_count\"),\n",
    "                 avg(\"weight\").alias(\"avg_weight\"))\n",
    "\n",
    "# Total txn count per merchant\n",
    "total_txns = joined.groupBy(\"merchant\").agg(count(\"*\").alias(\"total_txns\"))\n",
    "\n",
    "# Filter merchants with >= 50k transactions\n",
    "agg_filtered = agg.join(total_txns, \"merchant\") \\\n",
    "                  .filter((col(\"total_txns\") >= 50000) & (col(\"txn_count\") >= 169) & (col(\"avg_weight\") <= 23))\n",
    "\n",
    "print(f\"Pattern 1 detected rows: {agg_filtered.count()}\")\n",
    "agg_filtered.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06e065a7-49ea-4ce5-ac23-995bd3ca6165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pattern 2 - Debugging code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "517bcb4a-712c-4a5d-bb08-0968c06770bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, trim, lower, regexp_replace\n",
    "\n",
    "# Load batch data for debugging\n",
    "df = spark.read.table(\"googledrive.raw.transactions\")\n",
    "\n",
    "# Clean and cast columns\n",
    "df_clean = df.withColumn(\"merchant\", trim(lower(regexp_replace(col(\"merchant\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "             .withColumn(\"customer\", trim(lower(regexp_replace(col(\"customer\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "             .withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "             .dropna(subset=[\"merchant\", \"customer\", \"amount\"])\n",
    "\n",
    "# Aggregate per (merchant, customer)\n",
    "agg = df_clean.groupBy(\"merchant\", \"customer\") \\\n",
    "    .agg(avg(\"amount\").alias(\"avg_amount\"),\n",
    "         count(\"*\").alias(\"txn_count\"))\n",
    "\n",
    "# Filter per pattern 2 criteria\n",
    "p2_df = agg.filter((col(\"avg_amount\") < 23) & (col(\"txn_count\") >= 80))\n",
    "\n",
    "print(f\"Pattern 2 detected rows: {p2_df.count()}\")\n",
    "p2_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52ff6546-b082-4377-b64a-84ae1349eb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pattern 3 - Debugging code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35036353-28e0-47d5-affd-3c793e23478c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, trim, lower, regexp_replace\n",
    "\n",
    "# Load batch data\n",
    "df = spark.read.table(\"googledrive.raw.transactions\")\n",
    "\n",
    "# Clean columns: normalize strings, remove special chars, lowercase\n",
    "df_clean = df.withColumn(\"merchant\", trim(lower(regexp_replace(col(\"merchant\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "             .withColumn(\"customer\", trim(lower(regexp_replace(col(\"customer\"), \"[^a-zA-Z0-9]\", \"\")))) \\\n",
    "             .withColumn(\"gender\", trim(lower(regexp_replace(col(\"gender\"), \"'\", \"\"))))  # Remove single quotes\n",
    "\n",
    "# Normalize gender after cleanup\n",
    "gender_df = df_clean.withColumn(\"gender_norm\",\n",
    "    when(col(\"gender\").like(\"m%\"), \"Male\")\n",
    "    .when(col(\"gender\").like(\"f%\"), \"Female\")\n",
    "    .otherwise(None)\n",
    ").dropna(subset=[\"merchant\", \"customer\", \"gender_norm\"])\n",
    "\n",
    "# Show cleaned gender normalization\n",
    "gender_df.select(\"gender\", \"gender_norm\").distinct().show()\n",
    "\n",
    "# Distinct (merchant, customer, gender) to avoid duplicates\n",
    "distinct_gender = gender_df.select(\"merchant\", \"customer\", \"gender_norm\").distinct()\n",
    "\n",
    "# Group by merchant and pivot on gender, count distinct customers\n",
    "gender_counts = distinct_gender.groupBy(\"merchant\") \\\n",
    "    .pivot(\"gender_norm\", [\"Male\", \"Female\"]) \\\n",
    "    .count() \\\n",
    "    .na.fill(0)\n",
    "\n",
    "# Filter merchants for Pattern 3\n",
    "p3_df = gender_counts.filter((col(\"Female\") >= 100) & (col(\"Female\") < col(\"Male\")))\n",
    "\n",
    "print(f\"Pattern 3 detected merchants: {p3_df.count()}\")\n",
    "p3_df.show(10, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mechanism_Y_final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
