{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be3b9e50-8d81-4bd3-a1fb-7346a06fec95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import time\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "\n",
    "# Paths & Config\n",
    "CATALOG = \"testing\"\n",
    "SCHEMA = \"processed_data\"\n",
    "INPUT_PATH = \"/Volumes/testing/processed_data/staging_volume/staging_data/\"\n",
    "OUTPUT_PATH = \"/Volumes/testing/processed_data/staging_volume/output_detections_data/\"\n",
    "IMPORTANCE_TABLE = \"transactions.financial_transactions_google_drive.customer_importance\"\n",
    "PROGRESS_TABLE = f\"{CATALOG}.{SCHEMA}.detection_progress\"\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Load importance table once and clean\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "imp_df = spark.read.table(IMPORTANCE_TABLE) \\\n",
    "    .select(\"source\", \"target\", \"type_trans\", \"weight\") \\\n",
    "    .withColumn(\"source\", trim(lower(regexp_replace(col(\"source\"), \"'\", \"\")))) \\\n",
    "    .withColumn(\"target\", trim(lower(regexp_replace(col(\"target\"), \"'\", \"\")))) \\\n",
    "    .withColumn(\"type_trans\", trim(lower(regexp_replace(col(\"type_trans\"), \"'\", \"\")))) \\\n",
    "    .withColumn(\"type_trans\", regexp_replace(col(\"type_trans\"), \"_\", \"\")) \\\n",
    "    .withColumn(\"weight\", col(\"weight\").cast(\"double\")) \\\n",
    "    .dropna()\n",
    "\n",
    "# Get current IST time\n",
    "def ist_now():\n",
    "    return datetime.now(pytz.timezone(\"Asia/Kolkata\")).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Detect patterns\n",
    "def detect_patterns(df):\n",
    "    ystart = ist_now()\n",
    "\n",
    "    # Pattern 1: UPGRADE\n",
    "    joined = df.join(imp_df, (df[\"merchant\"] == imp_df[\"target\"]) &\n",
    "                               (df[\"customer\"] == imp_df[\"source\"]) &\n",
    "                               (df[\"category\"] == imp_df[\"type_trans\"]), \"inner\")\n",
    "    agg_p1 = joined.groupBy(\"merchant\", \"customer\").agg(\n",
    "        count(\"*\").alias(\"txn_count\"),\n",
    "        avg(\"weight\").alias(\"avg_weight\")\n",
    "    )\n",
    "    total_txn = joined.groupBy(\"merchant\").agg(count(\"*\").alias(\"total_txns\"))\n",
    "    agg_p1 = agg_p1.join(total_txn, \"merchant\").filter(col(\"total_txns\") >= 50000)\n",
    "\n",
    "    if agg_p1.count() > 0:\n",
    "        txn_thresh = agg_p1.approxQuantile(\"txn_count\", [0.9], 0.05)[0]\n",
    "        weight_thresh = agg_p1.approxQuantile(\"avg_weight\", [0.1], 0.05)[0]\n",
    "        p1 = agg_p1.filter((col(\"txn_count\") >= txn_thresh) & (col(\"avg_weight\") <= weight_thresh)) \\\n",
    "            .selectExpr(f\"'{ystart}' as YStartTime\", \"current_timestamp() as detectionTime\",\n",
    "                        \"'PatId1' as patternId\", \"'UPGRADE' as ActionType\",\n",
    "                        \"customer as customerName\", \"merchant as merchantId\")\n",
    "    else:\n",
    "        p1 = spark.createDataFrame([], \"YStartTime STRING, detectionTime TIMESTAMP, patternId STRING, ActionType STRING, customerName STRING, merchantId STRING\")\n",
    "\n",
    "    # Pattern 2: CHILD\n",
    "    p2 = df.groupBy(\"merchant\", \"customer\").agg(\n",
    "        avg(\"amount\").alias(\"avg_amount\"),\n",
    "        count(\"*\").alias(\"txn_count\")\n",
    "    ).filter((col(\"avg_amount\") < 23) & (col(\"txn_count\") >= 80)) \\\n",
    "     .selectExpr(f\"'{ystart}' as YStartTime\", \"current_timestamp() as detectionTime\",\n",
    "                 \"'PatId2' as patternId\", \"'CHILD' as ActionType\",\n",
    "                 \"customer as customerName\", \"merchant as merchantId\")\n",
    "\n",
    "    # Pattern 3: DEI-NEEDED\n",
    "    gender_df = df.withColumn(\"gender_norm\", when(col(\"gender\") == \"m\", \"Male\")\n",
    "                                           .when(col(\"gender\") == \"f\", \"Female\")\n",
    "                                           .otherwise(None))\n",
    "    p3_raw = gender_df.dropna(subset=[\"gender_norm\"]).select(\"merchant\", \"customer\", \"gender_norm\") \\\n",
    "                      .distinct().groupBy(\"merchant\") \\\n",
    "                      .pivot(\"gender_norm\", [\"Male\", \"Female\"]).count().na.fill(0)\n",
    "    p3 = p3_raw.filter((col(\"Female\") > 100) & (col(\"Female\") < col(\"Male\"))) \\\n",
    "        .selectExpr(f\"'{ystart}' as YStartTime\", \"current_timestamp() as detectionTime\",\n",
    "                    \"'PatId3' as patternId\", \"'DEI-NEEDED' as ActionType\",\n",
    "                    \"'' as customerName\", \"merchant as merchantId\")\n",
    "\n",
    "    detections = p1.unionByName(p2).unionByName(p3)\n",
    "    return detections\n",
    "\n",
    "# Read last progress\n",
    "def get_last_processed_ts():\n",
    "    if spark.catalog.tableExists(PROGRESS_TABLE):\n",
    "        return spark.read.table(PROGRESS_TABLE).agg({\"last_ingestion_timestamp\": \"max\"}).collect()[0][0]\n",
    "    return None\n",
    "\n",
    "# Save progress\n",
    "def update_progress(ts):\n",
    "    progress_df = spark.createDataFrame([(ts,)], [\"last_ingestion_timestamp\"])\n",
    "    progress_df.write.mode(\"overwrite\").saveAsTable(PROGRESS_TABLE)\n",
    "\n",
    "# --- Main Loop ---\n",
    "no_data_counter = 0\n",
    "MAX_RETRIES = 5\n",
    "\n",
    "while no_data_counter < MAX_RETRIES:\n",
    "    try:\n",
    "        last_ts = get_last_processed_ts()\n",
    "        df_batch = spark.read.format(\"delta\").load(INPUT_PATH)\n",
    "\n",
    "        if last_ts:\n",
    "            df_batch = df_batch.filter(col(\"ingestion_timestamp\") > lit(last_ts))\n",
    "\n",
    "        if df_batch.count() > 0:\n",
    "            print(f\"\\nüì• New records found since {last_ts}\")\n",
    "\n",
    "            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            # Clean & preprocess current chunk\n",
    "            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "            chunk = df_batch \\\n",
    "                .withColumn(\"merchant\", trim(lower(regexp_replace(col(\"merchant\"), \"'\", \"\")))) \\\n",
    "                .withColumn(\"customer\", trim(lower(regexp_replace(col(\"customer\"), \"'\", \"\")))) \\\n",
    "                .withColumn(\"category\", trim(lower(regexp_replace(col(\"category\"), \"'\", \"\")))) \\\n",
    "                .withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "                .withColumn(\"gender\", trim(lower(col(\"gender\")))) \\\n",
    "                .dropna(subset=[\"merchant\", \"customer\"])\n",
    "\n",
    "            result_df = detect_patterns(chunk)\n",
    "\n",
    "            if result_df.count() > 0:\n",
    "                result_df = result_df.withColumn(\"batch_id\", floor(monotonically_increasing_id() / 50))\n",
    "                result_df.write.format(\"delta\").mode(\"append\").partitionBy(\"batch_id\").save(OUTPUT_PATH)\n",
    "                print(f\"‚úÖ Wrote {result_df.count()} detections to {OUTPUT_PATH}\")\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è No detections found in this batch\")\n",
    "\n",
    "            latest_ts = df_batch.agg({\"ingestion_timestamp\": \"max\"}).collect()[0][0]\n",
    "            update_progress(latest_ts)\n",
    "\n",
    "            no_data_counter = 0\n",
    "        else:\n",
    "            no_data_counter += 1\n",
    "            print(f\"‚è≥ No new data to process... ({no_data_counter}/{MAX_RETRIES})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"\\nüõë No new data after 5 checks. Exiting detection loop.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5dd1d41-28c1-463e-873e-a628c9e84f41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(OUTPUT_PATH).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77380e9e-2093-41da-9655-2136ed2da9f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"/Volumes/testing/processed_data/staging_volume/output_detections_data/\"\n",
    "spark.read.format(\"delta\").load(OUTPUT_PATH).filter(col(\"ActionType\") == \"CHILD\").display()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mechanism_Y_final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
